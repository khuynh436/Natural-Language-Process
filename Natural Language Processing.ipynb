{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d508c1b-522b-447c-bca5-8be6c9ae5f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2af9bd4",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cbbe5-b5a7-4d2e-a4dd-130c9d8a8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set was already split into training and testing sets\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a52157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of training data \n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6267883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of testing data \n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d0290-84eb-4b39-9865-377b5f4b18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing training dataset\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44110f47-b57f-4f82-9dcf-f0472bad126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing testing dataset\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e67523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 of training set\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to calculate total of missing values in dataset\n",
    "def missing_values(df):\n",
    "    print(\"Number of records with missing location:\",df.location.isna().sum())\n",
    "    print(\"Number of records with missing keywords:\",df.keyword.isna().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values of training set\n",
    "missing_values(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values of testing set\n",
    "missing_values(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf607df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for keywords count\n",
    "keywords = df_train['keyword'].value_counts()\n",
    "print(keywords.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check location counts\n",
    "locations = df_train['location'].value_counts()\n",
    "print(locations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create barchart for locations in train set using seaborn\n",
    "sns.barplot(y=df_train['location'].value_counts()[:10].index,x=df_train['location'].value_counts()[:10],\n",
    "            orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feab3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create barchart for locations in test set using seaborn\n",
    "sns.barplot(y=df_test['location'].value_counts()[:10].index,x=df_test['location'].value_counts()[:10],\n",
    "            orient='h')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09f6f8b1",
   "metadata": {},
   "source": [
    "Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby \n",
    "df_train.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the tweets by disaster or not for the training data\n",
    "grouped = df_train.groupby(['target'])['text'].count()\n",
    "\n",
    "# plot the same as bar chart\n",
    "grouped.plot(kind='bar')\n",
    "plt.title('Disaster Tweet frequency chart for training data')\n",
    "plt.xlabel('Disaster or not')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9df0a81a",
   "metadata": {},
   "source": [
    "Tweet Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a961637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tweet lengths\n",
    "df_train['length'] = df_train['text'].apply(lambda x : len(x))\n",
    "df_train.head() # Check new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Tweet Lengths in training data\n",
    "# Code source: https://seaborn.pydata.org/generated/seaborn.displot.html\n",
    "\n",
    "sns.displot(data=df_train['length'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "df_train = df_train.drop(columns=['keyword', 'location', 'length'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fe98be9",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import string\n",
    "import string\n",
    "\n",
    "# Import nltk (Natural Language Toolkit)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# NLTK packages\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer as Stemmer\n",
    "\n",
    "# Code source for text preprocessing: \n",
    "# https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
    "\n",
    "def preprocess(text):\n",
    "    # lowercase \n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([t for t in text if t not in string.punctuation])\n",
    "    \n",
    "    # Removing stopwords since they do not add value to this analysis\n",
    "    # Code source: https://pythonprogramming.net/stop-words-nltk-tutorial/\n",
    "    text = [t for t in text.split() if t not in stopwords.words('english')]\n",
    "    \n",
    "    # Stemming is used to reducing words to their root \n",
    "    # Code source: https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "    stemmer = Stemmer()\n",
    "    text = [stemmer.stem(t) for t in text]\n",
    "    \n",
    "    # return text \n",
    "    return text\n",
    "\n",
    "# The function above is used to normalize and tokenize \n",
    "# texts that were found in the 'Text' column. \n",
    "# we cleaned the 'Text' column as much as we could by \n",
    "# using the NLTK (Natural Language Toolkit) library \n",
    "# that we found in their documentation (https://www.nltk.org/). \n",
    "# By cleaning this up, we are able reduce \n",
    "# the size of the vocab when we input into our machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d81ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with dataset, the first 20 rows\n",
    "df_train['text'][:20].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with dataset, the first 20 rows\n",
    "df_test['text'][:20].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit transform\n",
    "TFID = TfidfVectorizer(analyzer=preprocess)\n",
    "fit = TFID.fit_transform(df_train['text'])\n",
    "fits = TFID.fit_transform(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f64216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking values\n",
    "content = df_train.iloc[50]['text'] # Randomly chose 50th index\n",
    "print(content) # Print message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning texts to vectors\n",
    "\n",
    "# Code source:\n",
    "# https://github.com/scikit-learn/scikit-learn/blob/8c9c1f27b/sklearn/feature_extraction/text.py#L1470\n",
    "\n",
    "# Code source 2: \n",
    "# https://www.kaggle.com/code/jeffysonar/spam-filter-using-naive-bayes-classifier/notebook\n",
    "\n",
    "# Inputing \"content\" into transform function and adding to an array\n",
    "tfid = TFID.transform(['text']).toarray()[0]\n",
    "\n",
    "print('index\\tidf\\ttfidf\\tterm') # Print in this order\n",
    "\n",
    "# Loop function to assign different values to its term. \n",
    "for i in range(len(tfid)):\n",
    "    if tfid[i] != 0:\n",
    "        print(i, format(TFID\n",
    "                        .idf_[i], '.5f'), format(tfid[i], \n",
    "                                                 '.5f'), \n",
    "                        TFID.get_feature_names_out()[i],sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dd43043",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of text column\n",
    "training_texts = df_train['text']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(training_texts)\n",
    "y_train = df_train['target']\n",
    "X_test = vectorizer.transform(df_test['text'])\n",
    "\n",
    "print(X_train.size)\n",
    "print(y_train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5552bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ff67e-a59e-4d69-8f42-a86a64f24ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model for predicting diaster in tweets \n",
    "log = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "log.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "log_pred = log.predict(X_test)\n",
    "print(classification_report(y_test, log_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f3ac23c",
   "metadata": {},
   "source": [
    "# Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "# Fitting the model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Multinomial NB model on the testing set\n",
    "nb_pred = mnb.predict(X_test)\n",
    "print(classification_report(y_test, nb_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5842229",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe3d3-8d3a-4df4-8a03-ada3b0f382ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import silhouette_score, accuracy_score, classification_report\n",
    "\n",
    "# Building and training SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Fitting the model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the SVM model on the testing set\n",
    "svm_pred = svm.predict(X_test)\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8fd95e8",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2da5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Building Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "rf_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, rf_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbbac44d",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba48c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the  dataset\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer documentation: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# pad_sequences documentation: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "# Sequential model documentation: https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "# Embedding layer documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "# Dense layer documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "# Flatten layer documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten\n",
    "# Dropout layer documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df_train['text'])\n",
    "tokenizer.fit_on_texts(df_test['text'])\n",
    "train_sequences = tokenizer.texts_to_sequences(df_train['text'])\n",
    "train_padded = pad_sequences(train_sequences, padding='post', truncating='post')\n",
    "test_sequences = tokenizer.texts_to_sequences(df_test['text'])\n",
    "test_padded = pad_sequences(test_sequences, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfde4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Build model\n",
    "model.build(input_shape=(None, 21637)) \n",
    "\n",
    " # Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e97615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to  numpy arras to be compatible with keras\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', accuracy[0])\n",
    "print('Test accuracy:', accuracy[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
